{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluating Model Performance\n",
    "We'll investigate several different algorithms and determine which is best at modeling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation - Creating a Training and Predicting Pipeline\n",
    "\n",
    "To properly evaluate the performance of various models, we need to create a training and predicting pipeline that allows for quick and effective model training using various sizes of training data and perform predictions on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/features_final.csv', index_col='Id')\n",
    "target = pd.read_csv('../data/interim/target_log_xformed.csv', index_col='Id', squeeze=True)\n",
    "\n",
    "test_df = pd.read_csv('../data/processed/test_final.csv', index_col='Id')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(df, \n",
    "                                                    target, \n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 5)\n",
    "\n",
    "# Show the results of the split\n",
    "print(f'Training set has {X_train.shape[0]} samples.')\n",
    "print(f'Testing set has {X_val.shape[0]} samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(results, mae):\n",
    "\n",
    "def evaluate(results):\n",
    "    \"\"\"\n",
    "    Visualization code to display results of various learners.\n",
    "    \n",
    "    inputs:\n",
    "      - results: a list of supervised learners\n",
    "      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n",
    "      - mae: Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price\n",
    "    \"\"\"\n",
    "  \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(2, 3, figsize = (16,12))\n",
    "\n",
    "    # Constants\n",
    "    bar_width = 0.08\n",
    "#     colors = ['#A00000','#00A0A0','#00A000']\n",
    "    colors = sns.color_palette()\n",
    "    \n",
    "    # Super loop to plot four panels of data\n",
    "    for k, learner in enumerate(results.keys()):\n",
    "        for j, metric in enumerate(['train_time', 'rmse_train', 'mae_train', 'pred_time', 'rmse_test', 'mae_test']):\n",
    "            for i in range(3):\n",
    "                \n",
    "                # Creative plot code\n",
    "                ax[j//3, j%3].bar(i+k*bar_width, results[learner][i][metric], width = bar_width, color = colors[k])\n",
    "                ax[j//3, j%3].set_xticks([0.45, 1.45, 2.45])\n",
    "                ax[j//3, j%3].set_xticklabels([\"1%\", \"10%\", \"100%\"])\n",
    "                ax[j//3, j%3].set_xlabel(\"Training Set Size\")\n",
    "                ax[j//3, j%3].set_xlim((-0.1, 3.0))\n",
    "    \n",
    "    # Add unique y-labels\n",
    "    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n",
    "    ax[0, 1].set_ylabel(\"Root Mean Squared Error\")\n",
    "    ax[0, 2].set_ylabel(\"Mean Absolute Error\")\n",
    "    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n",
    "    ax[1, 1].set_ylabel(\"Root Mean Squared Error\")\n",
    "    ax[1, 2].set_ylabel(\"Mean Absolute Error\")\n",
    "    \n",
    "    # Add titles\n",
    "    ax[0, 0].set_title(\"Model Training\")\n",
    "    ax[0, 1].set_title(\"RMSE - Training Subset\")\n",
    "    ax[0, 2].set_title(\"MAE - Training Subset\")\n",
    "    ax[1, 0].set_title(\"Model Predicting\")\n",
    "    ax[1, 1].set_title(\"RMSE - Testing Set\")\n",
    "    ax[1, 2].set_title(\"MAE - Testing Subset\")\n",
    "\n",
    "    # Create patches for the legend\n",
    "    patches = []\n",
    "    for i, learner in enumerate(results.keys()):\n",
    "        patches.append(mpatches.Patch(color = colors[i], label = learner))\n",
    "    plt.legend(handles = patches, loc='upper center', bbox_to_anchor=(-.8, 2.53), ncol = 4, fontsize = 'x-large')\n",
    "    \n",
    "    # Aesthetics\n",
    "    plt.suptitle(\"Performance Metrics for Three Supervised Learning Models\", fontsize = 16, y = 1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_val, y_val): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_val: features testing set\n",
    "       - y_val: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Fit the learner to the training data\n",
    "    start = time() # Get start time\n",
    "    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # Get the predictions on the validation set(X_val) then training set\n",
    "    start = time() # Get start time\n",
    "    predictions_val = learner.predict(X_val)\n",
    "    predictions_train = learner.predict(X_train)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "    \n",
    "    # Compute RMSE on the training set\n",
    "    results['rmse_train'] = mean_squared_error(np.exp(y_train), np.exp(predictions_train), squared=False)\n",
    "    \n",
    "    # Compute RMSE on test set\n",
    "    results['rmse_test'] = mean_squared_error(np.exp(y_val), np.exp(predictions_val), squared=False)\n",
    "    \n",
    "    # Compute MAE on the the training set\n",
    "    results['mae_train'] = mean_absolute_error(np.exp(y_train), np.exp(predictions_train))\n",
    "        \n",
    "    # Compute MAE on the validation set\n",
    "    results['mae_test'] = mean_absolute_error(np.exp(y_val), np.exp(predictions_val))\n",
    "       \n",
    "    # Success\n",
    "    print(f'{learner.__class__.__name__} trained on {sample_size} samples.')\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Initial Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import supervised learning models\n",
    "\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# Initialize the models\n",
    "\n",
    "tr_model = TweedieRegressor(power=2, link='log')\n",
    "rf_model = RandomForestRegressor(random_state=5)\n",
    "adaBoost_model = AdaBoostRegressor(random_state=5)\n",
    "gb_model = GradientBoostingRegressor(random_state=5)\n",
    "ridge_model = Ridge(random_state=5)\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "algorithms = [tr_model, rf_model, adaBoost_model, gb_model,\n",
    "              ridge_model, knn_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm Comparison\n",
    "\n",
    "# Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "# samples_100 is the entire training set i.e. len(y_train)\n",
    "# samples_10 is 10% of samples_100\n",
    "# samples_1 is 1% of samples_100\n",
    "samples_100 = len(y_train)\n",
    "samples_10 = np.int_((samples_100 * 0.1))\n",
    "samples_1 = np.int_((samples_100 * 0.01))\n",
    "\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "# for rgsr in [rgsr_A, rgsr_B, rgsr_C, rgsr_D]:\n",
    "for rgsr in algorithms:\n",
    "    rgsr_name = rgsr.__class__.__name__\n",
    "    results[rgsr_name] = {}\n",
    "    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "        results[rgsr_name][i] = \\\n",
    "        train_predict(rgsr, samples, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Run metrics visualization for the three supervised learning models chosen\n",
    "evaluate(results)\n",
    "\n",
    "# Uncomment this line to save the figure.\n",
    "# plt.savefig('../reports/figures/Model_Comparison_Performance_Metrics.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the best model\n",
    "\n",
    "The GradientBoostRegressor Algorithm appears to return the lowest RMSE for our testing set. We'll try tuning it's parameters/hyperparameters to see if we can further improve our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized model\n",
      "------\n",
      "RMSE on testing data: 19933.2468\n",
      "\n",
      "Optimized Model\n",
      "------\n",
      "Final RMSE on the testing data: 18507.3550\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Initialize the regressor\n",
    "rgsr = GradientBoostingRegressor(random_state=5)\n",
    "\n",
    "# Create the parameters list to tune\n",
    "parameters = {'n_estimators': [10, 100, 1000], 'learning_rate': [0.001, 0.01, 0.1]}\n",
    "\n",
    "# Make an RMSE scoring object using make_scorer()\n",
    "scorer = make_scorer(mean_squared_error, squared=False, greater_is_better=False)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(estimator=rgsr, param_grid=parameters, scoring=scorer)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_rgsr = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (rgsr.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions = best_rgsr.predict(X_val)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"RMSE on testing data: {:.4f}\".format(mean_squared_error(np.exp(y_val), np.exp(predictions), squared=False)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final RMSE on the testing data: {:.4f}\".format(mean_squared_error(np.exp(y_val), np.exp(best_predictions), squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.9,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'ls',\n",
       " 'max_depth': 3,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 1000,\n",
       " 'n_iter_no_change': None,\n",
       " 'presort': 'deprecated',\n",
       " 'random_state': 5,\n",
       " 'subsample': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rgsr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = best_rgsr.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.Series(np.exp(final_preds), index=test_df.index, name='SalePrice')\n",
    "\n",
    "# Uncomment this line to export Results\n",
    "# preds.to_csv('../data/processed/predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HousingEnv",
   "language": "python",
   "name": "housingenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
